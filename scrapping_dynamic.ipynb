{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BeautifulSoup**\n",
    "BeautifulSoup is a Python library designed for quick turnaround projects like screen-scraping. It provides simple methods for navigating, searching, and modifying the parse tree of a webpage, making it immensely useful for web scraping purposes. However, BeautifulSoup works with static HTML content. When you make a request to a website, BeautifulSoup can parse the HTML content returned by that request. This is perfectly adequate for websites that serve all of their content in the initial page load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Limitations of BeautifulSoup**\n",
    "The main limitation of BeautifulSoup comes into play with dynamic web pages. Many modern websites use JavaScript to load data dynamically. This means the HTML content initially received might not contain all the information you see on the page when browsing with a web browser. Since BeautifulSoup cannot execute JavaScript, it cannot access content that is loaded dynamically after the initial page load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selenium**\n",
    "Selenium is a tool primarily used for automating web browsers. It allows you to programmatically control a web browser, such as Chrome or Firefox, enabling it to mimic human browsing behavior. When Selenium controls a browser, it can execute JavaScript and make additional HTTP requests that a site might make after the initial page load. This capability makes Selenium indispensable for scraping data from web pages that rely heavily on JavaScript for displaying content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why Combine Selenium with BeautifulSoup?**\n",
    "Dynamic Content: For web pages that load data dynamically with JavaScript, Selenium can be used to first load the page and execute any necessary JavaScript. Once the content is fully loaded on the page, you can then use BeautifulSoup to parse the HTML and extract the needed information.\n",
    "Interactivity: Some web pages require interaction, such as clicking buttons or filling out forms, to access the data. Selenium can automate these interactions, and once the desired content is rendered on the page, BeautifulSoup can be used to scrape it.\n",
    "Complex Navigation: In cases where you need to scrape data across multiple pages that require navigating through a complex web structure, Selenium can automate this process. After navigating to the specific page where the data is located, BeautifulSoup can be used for the extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    " while BeautifulSoup is excellent for parsing HTML and extracting data from static web pages, Selenium is needed to handle scenarios where the content is loaded dynamically or requires interaction. Combining Selenium with BeautifulSoup provides a powerful solution for web scraping challenges presented by modern, interactive websites. This combination allows for the automation of browser actions to access the content and the use of BeautifulSoup's parsing capabilities to efficiently extract and process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Importing Libraries:</b>\n",
    "\n",
    "- **from selenium import webdriver:** Imports the Selenium WebDriver to automate the web browser.\n",
    "- **from bs4 import BeautifulSoup:** Imports BeautifulSoup for HTML parsing.\n",
    "- **import pandas as pd:** Imports the Pandas library for data manipulation.\n",
    "- **from itertools import zip_longest:** Imports zip_longest from the itertools module to handle varying list sizes.\n",
    "- **import time:** Imports the time module for waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WebDriver Setup:**\n",
    "\n",
    "driver = webdriver.Chrome(): Initializes a Chrome WebDriver, which will be used to interact with the Chrome browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigating to the Website:**\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/laptops/pr?sid=6bo,b5g\"): Opens the specified URL in the Chrome browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/laptops/pr?sid=6bo,b5g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Waiting for Page to Load:**\n",
    "\n",
    "time.sleep(5): Pauses the execution for 5 seconds to allow the webpage to load. You may need to adjust this time based on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating BeautifulSoup Object:**\n",
    "\n",
    "the_soup = BeautifulSoup(driver.page_source, \"html.parser\"): Parses the HTML content of the page using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_soup = BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Laptop Details:**\n",
    "\n",
    "- names = [name.text for name in the_soup.find_all(class_=\"_4rR01T\")]: Extracts laptop names by finding HTML elements with the specified class.\n",
    "- prices = [price.text for price in the_soup.find_all(class_=\"_30jeq3 _1_WHN1\")]: Extracts laptop prices.\n",
    "- ratings = [rating.text for rating in the_soup.find_all(class_=\"_3LWZlK\")]: Extracts laptop ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name.text for name in the_soup.find_all(class_=\"_4rR01T\")]\n",
    "prices = [price.text for price in the_soup.find_all(class_=\"_30jeq3 _1_WHN1\")]\n",
    "ratings = [rating.text for rating in the_soup.find_all(class_=\"_3LWZlK\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating DataFrame with zip_longest:**\n",
    "\n",
    "- *data = {\"Name\": names, \"Price\": prices, \"Rating\": ratings}: Creates a dictionary with keys as column names and values as the extracted lists.\n",
    "- df = pd.DataFrame(zip_longest(*data.values()), columns=data.keys()): Uses zip_longest to align lists and create a Pandas DataFrame.\n",
    "- print(df): Ddisplays the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Name\": names, \"Price\": prices, \"Rating\": ratings}\n",
    "df = pd.DataFrame(zip_longest(*data.values()), columns=data.keys())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving DataFrame to CSV:**\n",
    "\n",
    "df.to_csv(\"laptops_data1.csv\", index=False): Optionally saves the DataFrame to a CSV file named \"laptops_data1.csv\" without including row indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"laptops_data1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Closing the WebDriver:**\n",
    "\n",
    "driver.quit(): Closes the Chrome browser once the scraping is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name    Price Rating\n",
      "0   Acer Chromebook Intel Celeron Dual Core N4000 ...  ₹19,771    3.5\n",
      "1   HP Victus AMD Ryzen 5 Hexa Core 5600H - (16 GB...  ₹50,999    3.9\n",
      "2   HP AMD Ryzen 3 Quad Core 5300U - (8 GB/512 GB ...  ₹32,990    4.2\n",
      "3   HP 15s Intel Core i3 12th Gen 1215U - (8 GB/51...  ₹36,990    4.2\n",
      "4   Acer Extensa (2023) AMD Ryzen 5 Quad Core 7520...  ₹29,990    4.1\n",
      "5   MSI Modern 14 AMD Ryzen 5 Hexa Core 7530U - (1...  ₹34,990    4.3\n",
      "6   HP 14s Intel Core i3 11th Gen 1115G4 - (8 GB/5...  ₹34,490    4.3\n",
      "7   CHUWI Intel Celeron Quad Core 12th Gen N100 - ...  ₹20,990    3.6\n",
      "8   DELL Intel Core i3 12th Gen 1215U - (8 GB/512 ...  ₹34,540    4.2\n",
      "9   SAMSUNG Galaxy Book 2 Intel Core i5 12th Gen 1...  ₹42,990    4.3\n",
      "10  CHUWI Intel Celeron Dual Core 10th Gen N4020 -...  ₹14,990    3.6\n",
      "11  CHUWI Intel Celeron Dual Core 11th Gen N4020 -...  ₹16,990    3.7\n",
      "12  Lenovo IdeaPad Slim 3 Intel Core i3 11th Gen 1...  ₹33,990    4.3\n",
      "13  HP 255 G9 AMD Ryzen 3 Dual Core AMD Ryzen3 325...  ₹25,990    4.3\n",
      "14  Lenovo IdeaPad 1 AMD Athlon Dual Core 7120U - ...  ₹26,990    4.2\n",
      "15  ASUS Vivobook 15 Intel Core i5 11th Gen 1135G7...  ₹40,990    4.3\n",
      "16  CHUWI Intel Celeron Dual Core 11th Gen N4020 -...  ₹18,990    3.7\n",
      "17  CHUWI Intel Core i5 10th Gen 1035G1 - (16 GB/5...  ₹34,990    4.3\n",
      "18  HP 2023 AMD Ryzen 3 Dual Core 3250U - (8 GB/51...  ₹28,990    4.2\n",
      "19  DELL Intel Core i3 12th Gen 1215U - (8 GB/512 ...  ₹33,890    4.2\n",
      "20  Lenovo IdeaPad Gaming 3 Intel Core i5 11th Gen...  ₹47,990    4.1\n",
      "21  CHUWI Intel Core i3 12th Gen Intel Core i3-121...  ₹27,990    4.2\n",
      "22  walker Intel Celeron Dual Core N4020 - (4 GB/1...  ₹12,990    3.8\n",
      "23  ASUS Vivobook 15 Intel Core i3 11th Gen 1115G4...  ₹29,990    4.3\n",
      "24                                               None     None    4.2\n",
      "25                                               None     None      4\n",
      "26                                               None     None      2\n",
      "27                                               None     None    4.3\n",
      "28                                               None     None      1\n",
      "29                                               None     None      5\n",
      "30                                               None     None    4.3\n",
      "31                                               None     None      5\n",
      "32                                               None     None      3\n",
      "33                                               None     None    4.2\n",
      "34                                               None     None      2\n",
      "35                                               None     None      5\n",
      "36                                               None     None    4.2\n",
      "37                                               None     None      5\n",
      "38                                               None     None      5\n"
     ]
    }
   ],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
